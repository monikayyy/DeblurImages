{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monikayyy/DeblurImages/blob/main/DeBlur_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vVfGNSDq8yeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e32ec46-3705-47a8-c883-f6d03bfcf708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OIQoRyk37iAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568adf98-9ce7-4802-e1b8-420fa98f68e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (8.4.0)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.7.5\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "!pip install natsort\n",
        "!pip install torchmetrics\n",
        "!pip install lmdb\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "RnXdEzrWPvbW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWnbcFmZf4qh"
      },
      "source": [
        "# Load TransformerPerceptualLoss module\n",
        "\n",
        "Reference: Image Deblurring by Exploring In-depth Properties of Transformer\n",
        "\n",
        "- changes in double_mae_model: remove qv_scale\n",
        "- changes in deblurloss: add a check for dict extraction from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TdE66cTH6R4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ec9f6e-e050-4482-bcaf-20e8289c80c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TransformerPerceptualLoss'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 137 (delta 11), reused 2 (delta 2), pack-reused 115 (from 1)\u001b[K\n",
            "Receiving objects: 100% (137/137), 60.64 KiB | 20.21 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/erfect2020/TransformerPerceptualLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uqRuREJviFMA"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/TransformerPerceptualLoss /content/drive/MyDrive/REDS/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJuNGj3EhCT4"
      },
      "source": [
        "# Fetch data\n",
        "- REDS dataset\n",
        "  - train\n",
        "  - val\n",
        "  - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7pNRHvW8s9q9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968,
          "referenced_widgets": [
            "3b656d1f885941ca9c54702135e7f17b",
            "ee4b27b0639449bba23baf7090353a03",
            "40049daeb03144388c16deb3a9209053",
            "72ccca726ad4423497fc1e01be0d1e37",
            "a0a188b95bc047efb38d7c8df7ffb075",
            "7e28eba5cd694a75a69f8110546e9c70",
            "8a86ab0af4624f268f397aac079af452",
            "81cc7e35b8fb42db8ab3072f2dccc37e",
            "120783e41b7f4ef986c6a185c65b8749",
            "0b0624c5f3954d57bee57c4fe051839c",
            "4817bb626918403d9d87b0cdbe785baf"
          ]
        },
        "outputId": "1377a9a1-ca22-40a6-b53d-5597ded3e509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: train_blur.zip from snah/REDS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_blur.zip:   0%|          | 0.00/28.3G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b656d1f885941ca9c54702135e7f17b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         download_files(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mxet_download_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3829581210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading: {file_path} from {repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         local_path = hf_hub_download(\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    985\u001b[0m             )\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         return _hf_hub_download_to_local_dir(\n\u001b[0m\u001b[1;32m    988\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mlocal_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_local_dir\u001b[0;34m(local_dir, repo_id, repo_type, filename, revision, endpoint, etag_timeout, headers, proxies, token, cache_dir, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# delete outdated file first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1298\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincomplete_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxet_file_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_xet_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xet Storage is enabled for this repo. Downloading file from Xet Storage..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m             xet_get(\n\u001b[0m\u001b[1;32m   1721\u001b[0m                 \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincomplete_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0mxet_file_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxet_file_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mprogress_cm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprogress_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "repo_id = \"snah/REDS\"\n",
        "repo_type = \"dataset\"\n",
        "\n",
        "files_to_download = [\n",
        "    \"train_blur.zip\",\n",
        "    \"train_sharp.zip\",\n",
        "    \"val_blur.zip\",\n",
        "    \"val_sharp.zip\",\n",
        "    \"test_blur.zip\"\n",
        "]\n",
        "\n",
        "downloaded_files = []\n",
        "for file_path in files_to_download:\n",
        "    print(f\"Downloading: {file_path} from {repo_id}\")\n",
        "    try:\n",
        "        local_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=file_path,\n",
        "            repo_type=repo_type,\n",
        "            # token=True, # Use if you logged in via notebook_login() or HF_TOKEN secret\n",
        "            local_dir='.',          # Optional: Download directly to current dir (./content/) instead of cache\n",
        "            local_dir_use_symlinks=False # Recommended with local_dir='.' to avoid symlinks\n",
        "        )\n",
        "        downloaded_files.append(local_path)\n",
        "        print(f\"Downloaded to: {local_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {file_path}: {e}\")\n",
        "\n",
        "print(\"\\nFinished download attempts.\")\n",
        "print(\"Downloaded file paths:\", downloaded_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RODX5qhDiOJc"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/train_blur.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/train_sharp.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/val_blur.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/val_sharp.zip /content/drive/MyDrive/REDS/\n",
        "!cp -r /content/test_blur.zip /content/drive/MyDrive/REDS/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTViGisbt1pp"
      },
      "source": [
        "# Dataset Preparation\n",
        "- select pairs of blur and sharp image files from train and val\n",
        "- 24000 pairs in train\n",
        "- 3000 pairs in val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E2G-Qp5AvDMy"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import random\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import glob\n",
        "\n",
        "class PairedZipDataset(Dataset):\n",
        "    def __init__(self, blur_zip_path, sharp_zip_path, transform=None):\n",
        "        self.blur_zip = zipfile.ZipFile(blur_zip_path, 'r')\n",
        "        self.sharp_zip = zipfile.ZipFile(sharp_zip_path, 'r')\n",
        "        self.transform = transform\n",
        "        self.blur_files = sorted([f for f in self.blur_zip.namelist() if not f.endswith('/')])\n",
        "        self.sharp_files = sorted([f for f in self.sharp_zip.namelist() if not f.endswith('/')])\n",
        "        self.blur_folders = self.group_by_IMG_folder(self.blur_files)\n",
        "        self.sharp_folders = self.group_by_IMG_folder(self.sharp_files)\n",
        "        self.paired_files = []\n",
        "        paired_count = 0\n",
        "\n",
        "        for folder in self.blur_folders.keys():\n",
        "            if folder in self.sharp_folders.keys():\n",
        "                blur_imgs = self.blur_folders[folder]\n",
        "                sharp_imgs = self.sharp_folders[folder]\n",
        "                if not blur_imgs or not blur_imgs:\n",
        "                  continue\n",
        "\n",
        "                blur_map = {os.path.splitext(os.path.basename(p))[0]: p for p in blur_imgs}\n",
        "                sharp_map = {os.path.splitext(os.path.basename(p))[0]: p for p in sharp_imgs}\n",
        "                common_filenames = sorted(list(blur_map.keys() & sharp_map.keys())) # Intersection of keys\n",
        "\n",
        "                if not common_filenames:\n",
        "                    continue\n",
        "\n",
        "                # folder_pairs = []\n",
        "                for fname in common_filenames:\n",
        "                    self.paired_files.append((blur_map[fname], sharp_map[fname]))\n",
        "                    paired_count += 1\n",
        "        print(f\"Total paired files added: {paired_count}\")\n",
        "        if not self.paired_files:\n",
        "            print(f\"Warning: No paired files found after matching filenames. Check filenames and structure in zip files.\")\n",
        "\n",
        "    def group_by_IMG_folder(self, file_paths):\n",
        "        folder_dict = {}\n",
        "        for file_path in file_paths:\n",
        "            folder = os.path.basename(os.path.dirname(file_path))\n",
        "            folder_dict.setdefault(folder, []).append(file_path)\n",
        "        return folder_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paired_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_file, sharp_file = self.paired_files[idx]\n",
        "\n",
        "\n",
        "        blur_img = self.load_image_from_zip(self.blur_zip, blur_file)\n",
        "        sharp_img = self.load_image_from_zip(self.sharp_zip, sharp_file)\n",
        "\n",
        "        if self.transform:\n",
        "            blur_img = self.transform(blur_img)\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "\n",
        "        return blur_img, sharp_img\n",
        "\n",
        "    def load_image_from_zip(self, zip_file, file):\n",
        "        with zip_file.open(file) as file:\n",
        "            img_data = file.read()\n",
        "            img = Image.open(BytesIO(img_data)).convert('RGB')\n",
        "        return img\n",
        "\n",
        "    def load_image_from_idx(self, idx):\n",
        "        blur_img, sharp_img = self.paired_files[idx]\n",
        "\n",
        "        blur = self.load_image_from_zip(self.blur_zip, blur_img)\n",
        "        sharp = self.load_image_from_zip(self.sharp_zip, sharp_img)\n",
        "        return blur, sharp\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            if self.blur_zip:\n",
        "                self.blur_zip.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error closing blur zip: {e}\")\n",
        "        try:\n",
        "            if self.sharp_zip:\n",
        "                self.sharp_zip.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error closing sharp zip: {e}\")\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameters"
      ],
      "metadata": {
        "id": "sE3DUnpi2gNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/train_blur.zip'\n",
        "SHARP_ZIP_PATH = '/content/drive/MyDrive/REDS/train_sharp.zip'\n",
        "VAL_BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/val_blur.zip'\n",
        "VAL_SHARP_ZIP_PATH = '/content/drive/MyDrive/REDS/val_sharp.zip'\n",
        "TEST_BLUR_ZIP_PATH = '/content/drive/MyDrive/REDS/test_blur.zip'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/REDS/mae_vit_output'\n",
        "\n",
        "#Pretrained MAE vision transformer for feature extraction\n",
        "# PRETRAINED_WEIGHTS_PATH = '/content/drive/MyDrive/REDS/pytorch_model.bin'\n",
        "\n",
        "IMG_SIZE = 224\n",
        "VAL_BATCH_SIZE = 15\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR_DECAY_EPOCHS = 40\n",
        "LR_FINAL = 2e-5\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "NUM_WORKERS = 1\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "6LT7hvU52XID"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load modules from TransformerPerceptualLoss for feature extraction and perceptual loss calculation\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/models')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/loss')\n",
        "sys.path.append('/content/drive/MyDrive/REDS/TransformerPerceptualLoss/utils')\n",
        "print(sys.path)"
      ],
      "metadata": {
        "id": "RfFWo4nM2yhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387cbedd-4949-46af-dcd3-b3624feada97"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython', '/tmp/tmpocrc1xyp', '/content/Restormer', '/content/drive/MyDrive/REDS/TransformerPerceptualLoss', '/content/drive/MyDrive/REDS/TransformerPerceptualLoss/models', '/content/drive/MyDrive/REDS/TransformerPerceptualLoss/loss', '/content/drive/MyDrive/REDS/TransformerPerceptualLoss/utils']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c1ui-o-5g1h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    dataset = worker_info.dataset\n",
        "    if hasattr(dataset, '_open_zips'):\n",
        "         dataset._open_zips()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = PairedZipDataset(\n",
        "    blur_zip_path=BLUR_ZIP_PATH,\n",
        "    sharp_zip_path=SHARP_ZIP_PATH,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = PairedZipDataset(\n",
        "    blur_zip_path=VAL_BLUR_ZIP_PATH,\n",
        "    sharp_zip_path=VAL_SHARP_ZIP_PATH,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True, # Use True if DEVICE is 'cuda'\n",
        "        drop_last=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False # Can speed up epoch start\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=VAL_BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True, # Use True if DEVICE is 'cuda'\n",
        "        drop_last=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False # Can speed up epoch start\n",
        "    )\n",
        "\n",
        "print(\"DataLoader created.\")"
      ],
      "metadata": {
        "id": "nuF4Ph7D1igF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae002575-adc3-495c-a227-cfd27b89fab5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total paired files added: 24000\n",
            "Total paired files added: 3000\n",
            "DataLoader created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize blur, sharp, and restored images\n",
        "Visualize few images from every validation batch to monitor the results of training"
      ],
      "metadata": {
        "id": "npqvOiAD05Y4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwwluk5jAtl"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_validation_batch(blur_batch, pred_batch, sharp_batch, num_images=4, title_prefix=\"\"):\n",
        "\n",
        "    if not all(isinstance(t, torch.Tensor) for t in [blur_batch, pred_batch, sharp_batch]):\n",
        "        print(\"Warning: All input batches must be PyTorch Tensors.\")\n",
        "        return\n",
        "\n",
        "    if not (blur_batch.ndim == 4 and pred_batch.ndim == 4 and sharp_batch.ndim == 4):\n",
        "        print(\"Warning: All input batches must be 4D tensors (B, C, H, W).\")\n",
        "        return\n",
        "\n",
        "    num_to_show = min(num_images, blur_batch.shape[0], pred_batch.shape[0], sharp_batch.shape[0])\n",
        "\n",
        "    if num_to_show == 0:\n",
        "        print(\"Warning: No images to show (batch size might be 0, num_images=0, or mismatched batch sizes).\")\n",
        "        return\n",
        "\n",
        "    # Detach tensors from the computation graph and select the subset to show\n",
        "    blur_imgs_t = blur_batch[:num_to_show].detach()\n",
        "    pred_imgs_t = pred_batch[:num_to_show].detach()\n",
        "    sharp_imgs_t = sharp_batch[:num_to_show].detach()\n",
        "\n",
        "    # Clamp image values to [0, 1] for proper display\n",
        "    # (Important if model outputs are not strictly in this range)\n",
        "    blur_imgs_t = torch.clamp(blur_imgs_t, 0, 1)\n",
        "    pred_imgs_t = torch.clamp(pred_imgs_t, 0, 1)\n",
        "    sharp_imgs_t = torch.clamp(sharp_imgs_t, 0, 1)\n",
        "\n",
        "    # Create subplots: num_to_show rows, 3 columns (Input, Predicted, Ground Truth)\n",
        "    # Adjust figsize as needed\n",
        "    fig, axes = plt.subplots(num_to_show, 3, figsize=(12, num_to_show * 4))\n",
        "\n",
        "    # If num_to_show is 1, axes is a 1D array, so we need to handle it\n",
        "    if num_to_show == 1:\n",
        "        axes = axes.reshape(1, -1) # Reshape to (1, 3) to make indexing consistent\n",
        "\n",
        "    for i in range(num_to_show):\n",
        "        # Convert tensors to NumPy arrays for matplotlib\n",
        "        # Permute from (C, H, W) to (H, W, C) and move to CPU\n",
        "        blur_np = blur_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "        pred_np = pred_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "        sharp_np = sharp_imgs_t[i].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "        # --- Column 0: Input Blurry ---\n",
        "        axes[i, 0].imshow(blur_np)\n",
        "        axes[i, 0].set_title(f\"Input Blurry {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # --- Column 1: Predicted Sharp ---\n",
        "        axes[i, 1].imshow(pred_np)\n",
        "        axes[i, 1].set_title(f\"Predicted Sharp {i+1}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # --- Column 2: Ground Truth Sharp ---\n",
        "        axes[i, 2].imshow(sharp_np)\n",
        "        axes[i, 2].set_title(f\"Ground Truth Sharp {i+1}\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.suptitle(title_prefix, fontsize=14, y=1.0) # y=1.0 might be slightly high, adjust if needed\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97]) # rect to make space for suptitle\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation Loss\n",
        "Functions for calculating validation loss every epoch while training the models\n",
        "\n",
        "\n",
        "\n",
        "- restormer\n",
        "- deepdeblur"
      ],
      "metadata": {
        "id": "IEOd6OVlxfPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Logic for Restormer\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "import gc\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, device, criterion, visualize=False, num_images_to_show=4, epoch_num=None):\n",
        "    model.eval()\n",
        "    # Initialize validation loss\n",
        "    val_loss = 0.0\n",
        "    visualized_this_epoch = not visualize\n",
        "    # Handle empty dataloader\n",
        "    if not dataloader:\n",
        "        print(\"Validation dataloader is empty.\")\n",
        "        model.train()\n",
        "        return 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Validating Epoch {epoch_num if epoch_num is not None else 'N/A'}\", leave=False)\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "        blur_imgs = blur_imgs.to(device)\n",
        "        sharp_imgs = sharp_imgs.to(device)\n",
        "\n",
        "        recover_img = model(blur_imgs)\n",
        "\n",
        "        if not visualized_this_epoch:\n",
        "            print(f\"\\nVisualizing Validation Batch {batch_idx} (Epoch {epoch_num if epoch_num is not None else 'N/A'})...\")\n",
        "            title = f\"Validation - Epoch {epoch_num}\" if epoch_num is not None else \"Validation\"\n",
        "            visualize_validation_batch(blur_imgs.cpu(),\n",
        "                                       recover_img.cpu(),\n",
        "                                       sharp_imgs.cpu(),\n",
        "                                       num_images=num_images_to_show,\n",
        "                                       title_prefix=title)\n",
        "            visualized_this_epoch = True # Ensure visualization happens only once per epoch call\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        losses = criterion(recover_img, sharp_imgs)\n",
        "        grad_loss = losses[\"total_loss\"]\n",
        "\n",
        "        current_batch_loss = grad_loss.item()\n",
        "        val_loss += current_batch_loss\n",
        "\n",
        "        # Display current batch loss and running average epoch loss\n",
        "        progress_bar.set_postfix(\n",
        "            BatchLoss=f\"{current_batch_loss:.4f}\",\n",
        "            AvgEpochLoss=f\"{val_loss / (batch_idx + 1):.4f}\"\n",
        "        )\n",
        "\n",
        "        del recover_img, losses, grad_loss, current_batch_loss\n",
        "\n",
        "\n",
        "    if len(dataloader) > 0:\n",
        "        avg_val_loss = val_loss / len(dataloader)\n",
        "    else:\n",
        "        avg_val_loss = 0.0\n",
        "\n",
        "    try:\n",
        "        del blur_imgs, sharp_imgs\n",
        "    except NameError:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "Y2BaQztr5dI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS9K9XJ647Je"
      },
      "outputs": [],
      "source": [
        "#Validation Logic for DeepDeblur\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "import gc\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, device, criterion, visualize=False, num_images_to_show=4, epoch_num=None):\n",
        "    model.eval()\n",
        "    # Initialize validation loss\n",
        "    val_loss = 0.0\n",
        "    visualized_this_epoch = not visualize\n",
        "    # Handle empty dataloader\n",
        "    if not dataloader:\n",
        "        print(\"Validation dataloader is empty.\")\n",
        "        model.train()\n",
        "        return 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Validating Epoch {epoch_num if epoch_num is not None else 'N/A'}\", leave=False)\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "        blur_imgs = blur_imgs.to(device)\n",
        "        sharp_imgs = sharp_imgs.to(device)\n",
        "\n",
        "        sharp_gt_half = F.interpolate(sharp_imgs, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter = F.interpolate(sharp_gt_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        fine_out, mid_out, coarse_out = model(blur_imgs)\n",
        "\n",
        "        sharp_gt_half_224 = F.interpolate(sharp_gt_half, size = 224, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter_224 = F.interpolate(sharp_gt_quarter, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        mid_out_224 = F.interpolate(mid_out, size = 224, mode='bilinear', align_corners=False)\n",
        "        coarse_out_224 = F.interpolate(coarse_out, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        if not visualized_this_epoch:\n",
        "            print(f\"\\nVisualizing Validation Batch {batch_idx} (Epoch {epoch_num if epoch_num is not None else 'N/A'})...\")\n",
        "            title = f\"Validation - Epoch {epoch_num}\" if epoch_num is not None else \"Validation\"\n",
        "            visualize_validation_batch(blur_imgs.cpu(),\n",
        "                                       fine_out.cpu(),\n",
        "                                       sharp_imgs.cpu(),\n",
        "                                       num_images=num_images_to_show,\n",
        "                                       title_prefix=title)\n",
        "            visualized_this_epoch = True # Ensure visualization happens only once per epoch call\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        loss_fine = criterion(fine_out, sharp_imgs)\n",
        "        loss_mid = criterion(mid_out_224, sharp_gt_half_224)\n",
        "        loss_coarse = criterion(coarse_out_224, sharp_gt_quarter_224)\n",
        "        # Combining the fine, mid and coarse losses\n",
        "        losses = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "\n",
        "        current_batch_loss = losses.item()\n",
        "        val_loss += current_batch_loss\n",
        "\n",
        "        # Display current batch loss and running average epoch loss\n",
        "        progress_bar.set_postfix(\n",
        "            BatchLoss=f\"{current_batch_loss:.4f}\",\n",
        "            AvgEpochLoss=f\"{val_loss / (batch_idx + 1):.4f}\"\n",
        "        )\n",
        "\n",
        "        del fine_out, mid_out, coarse_out, mid_out_224, coarse_out_224, sharp_gt_half, sharp_gt_quarter, sharp_gt_half_224, sharp_gt_quarter_224\n",
        "\n",
        "\n",
        "    if len(dataloader) > 0:\n",
        "        avg_val_loss = val_loss / len(dataloader)\n",
        "    else:\n",
        "        avg_val_loss = 0.0\n",
        "\n",
        "    try:\n",
        "        del blur_imgs, sharp_imgs\n",
        "    except NameError:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return avg_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ANotA0t03bYt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-V45465t_Yc"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEy_zpa67ZfO"
      },
      "source": [
        "##Setting up Restormer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ChdMuuBAIH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65519219-53a9-4b87-955f-9b2a723eba48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Cloning into 'Restormer'...\n",
            "remote: Enumerating objects: 312, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 312 (delta 74), reused 72 (delta 72), pack-reused 197 (from 2)\u001b[K\n",
            "Receiving objects: 100% (312/312), 1.55 MiB | 26.41 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n",
            "/content/Restormer\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "\n",
        "if os.path.isdir('Restormer'):\n",
        "  !rm -r Restormer\n",
        "\n",
        "# Clone Restormer\n",
        "!git clone https://github.com/swz30/Restormer.git\n",
        "%cd Restormer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Restormer')"
      ],
      "metadata": {
        "id": "93semyl3263Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PJtmVds-BQom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e9f217-c125-428b-ee17-7caaca1dc5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-12 00:08:24--  https://github.com/swz30/Restormer/releases/download/v1.0/single_image_defocus_deblurring.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:7: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:9: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:11: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:13: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:7: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:9: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:11: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:13: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "/tmp/ipython-input-3640428238.py:7: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if task is 'Real_Denoising':\n",
            "/tmp/ipython-input-3640428238.py:9: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if task is 'Single_Image_Defocus_Deblurring':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/418793252/29507e7d-b992-4c77-a4f9-ec3fb3b555bf?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-12T01%3A06%3A33Z&rscd=attachment%3B+filename%3Dsingle_image_defocus_deblurring.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-12T00%3A05%3A46Z&ske=2026-01-12T01%3A06%3A33Z&sks=b&skv=2018-11-09&sig=m2pqOOdoXRviDyGHF1zrWFybDZ8VqM7VvsJ7%2BH0BjCg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODE3ODMwNCwibmJmIjoxNzY4MTc2NTA0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.hTuPK9HFJDpMur7DkNI4cs7Rx0JwJE4QuL6tXEH2AL0&response-content-disposition=attachment%3B%20filename%3Dsingle_image_defocus_deblurring.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-01-12 00:08:24--  https://release-assets.githubusercontent.com/github-production-release-asset/418793252/29507e7d-b992-4c77-a4f9-ec3fb3b555bf?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-12T01%3A06%3A33Z&rscd=attachment%3B+filename%3Dsingle_image_defocus_deblurring.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-12T00%3A05%3A46Z&ske=2026-01-12T01%3A06%3A33Z&sks=b&skv=2018-11-09&sig=m2pqOOdoXRviDyGHF1zrWFybDZ8VqM7VvsJ7%2BH0BjCg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODE3ODMwNCwibmJmIjoxNzY4MTc2NTA0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.hTuPK9HFJDpMur7DkNI4cs7Rx0JwJE4QuL6tXEH2AL0&response-content-disposition=attachment%3B%20filename%3Dsingle_image_defocus_deblurring.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104700429 (100M) [application/octet-stream]\n",
            "Saving to: ‘Defocus_Deblurring/pretrained_models/single_image_defocus_deblurring.pth’\n",
            "\n",
            "        single_imag  85%[================>   ]  85.19M  4.45MB/s    eta 4s     ^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3640428238.py:11: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if task is 'Motion_Deblurring':\n",
            "/tmp/ipython-input-3640428238.py:13: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if task is 'Deraining':\n"
          ]
        }
      ],
      "source": [
        "# task = 'Real_Denoising'\n",
        "task = 'Single_Image_Defocus_Deblurring'\n",
        "# task = 'Motion_Deblurring'\n",
        "# task = 'Deraining'\n",
        "\n",
        "# Download the pre-trained models\n",
        "if task is 'Real_Denoising':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/real_denoising.pth -P Denoising/pretrained_models\n",
        "if task is 'Single_Image_Defocus_Deblurring':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/single_image_defocus_deblurring.pth -P Defocus_Deblurring/pretrained_models\n",
        "if task is 'Motion_Deblurring':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/motion_deblurring.pth -P Motion_Deblurring/pretrained_models\n",
        "if task is 'Deraining':\n",
        "  !wget https://github.com/swz30/Restormer/releases/download/v1.0/deraining.pth -P Deraining/pretrained_models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Pretrained Restormer\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from runpy import run_path\n",
        "from skimage import img_as_ubyte\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from basicsr.models.archs.restormer_arch import Restormer\n",
        "\n",
        "def get_weights_and_parameters(task, parameters):\n",
        "    if task == 'Motion_Deblurring':\n",
        "        weights = os.path.join('Motion_Deblurring', 'pretrained_models', 'motion_deblurring.pth')\n",
        "    elif task == 'Single_Image_Defocus_Deblurring':\n",
        "        weights = os.path.join('Defocus_Deblurring', 'pretrained_models', 'single_image_defocus_deblurring.pth')\n",
        "    elif task == 'Deraining':\n",
        "        weights = os.path.join('Deraining', 'pretrained_models', 'deraining.pth')\n",
        "    elif task == 'Real_Denoising':\n",
        "        weights = os.path.join('Denoising', 'pretrained_models', 'real_denoising.pth')\n",
        "        parameters['LayerNorm_type'] =  'BiasFree'\n",
        "    return weights, parameters\n",
        "\n",
        "\n",
        "\n",
        "# use pretrained restormer's weights\n",
        "\n",
        "#model_path = '/content/Restormer/Defocus_Deblurring/pretrained_models/single_image_defocus_deblurring.pth'\n",
        "\n",
        "# checkpoint = torch.load(model_path, map_location='cpu')\n",
        "# try:\n",
        "#     model.load_state_dict(checkpoint[\"params\"], strict=True)\n",
        "#     print(f\"Loaded weights from {model_path} using 'params' key.\")\n",
        "# except KeyError:\n",
        "#     try:\n",
        "#         model.load_state_dict(checkpoint, strict=True)\n",
        "#         print(f\"Loaded weights directly from {model_path}.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading state dict: {e}\")\n",
        "#         print(\"You might need to inspect the checkpoint keys or adjust loading logic.\")\n"
      ],
      "metadata": {
        "id": "xKpIN8zy3qV6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0bbEbRZjZ0n"
      },
      "source": [
        "##Setting up DeepDeblur model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_qbOw2whjVa7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------------------------------------\n",
        "# Residual Block\n",
        "# ---------------------------------------\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_feats, num_feats, 3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(num_feats, num_feats, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv2(self.relu(self.conv1(x)))\n",
        "\n",
        "# ---------------------------------------\n",
        "# Single-Scale Deblurring Network\n",
        "# ---------------------------------------\n",
        "class SingleScaleDeblurNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_feats=64, num_blocks=8):\n",
        "        super().__init__()\n",
        "        self.head = nn.Conv2d(in_channels, num_feats, kernel_size=3, padding=1)\n",
        "        self.body = nn.Sequential(*[ResBlock(num_feats) for _ in range(num_blocks)])\n",
        "        self.tail = nn.Conv2d(num_feats, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.head(x)\n",
        "        feat = self.body(feat)\n",
        "        out = self.tail(feat)\n",
        "        return out\n",
        "\n",
        "# ---------------------------------------\n",
        "# Multi-Scale Deblurring Network (DeepDeblurMS)\n",
        "# ---------------------------------------\n",
        "class DeepDeblurMS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each stage expects concatenated inputs → 6 channels: [blur, upsampled_output]\n",
        "        self.coarse_net = SingleScaleDeblurNet(in_channels=6)\n",
        "        self.middle_net = SingleScaleDeblurNet(in_channels=6)\n",
        "        self.fine_net = SingleScaleDeblurNet(in_channels=6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create image pyramid\n",
        "        x_half = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        x_quarter = F.interpolate(x_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Coarse scale input: duplicate x_quarter to simulate [blur, blur]\n",
        "        coarse_input = torch.cat([x_quarter, x_quarter], dim=1)\n",
        "        coarse_out = self.coarse_net(coarse_input)\n",
        "        up_coarse = F.interpolate(coarse_out, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Middle scale input: [blur_half, upsampled_coarse]\n",
        "        mid_input = torch.cat([x_half, up_coarse], dim=1)\n",
        "        mid_out = self.middle_net(mid_input)\n",
        "        up_mid = F.interpolate(mid_out, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Fine scale input: [blur_full, upsampled_middle]\n",
        "        fine_input = torch.cat([x, up_mid], dim=1)\n",
        "        fine_out = self.fine_net(fine_input)\n",
        "\n",
        "        return fine_out, mid_out, coarse_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7p3_yJaiId8"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bKAHv4WV6iBj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from natsort import natsorted\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pdb import set_trace as stx\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q4bvSCz9pTWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5bbc41-53fe-45ad-dd99-5b72929b487f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training logic for Restormer model"
      ],
      "metadata": {
        "id": "B-tIt9Kp15II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Restormer on Perceptual Loss\n",
        "\n",
        "import loss.deblur_loss as deblur_loss\n",
        "from deblur_loss import ReconstructPerceptualLoss as ReconstructLoss\n",
        "import importlib\n",
        "from utils import pos_embed\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block as TimmBlock\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from utils.pos_embed import get_2d_sincos_pos_embed\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "LOAD_PRETRAINED = True\n",
        "run_validation = True\n",
        "\n",
        "MODEL_NAME = 'Restormer'\n",
        "\n",
        "# Restormer for deblur task\n",
        "\n",
        "parameters = {'inp_channels':3, 'out_channels':3, 'dim':48, 'num_blocks':[4,6,6,8], 'num_refinement_blocks':4, 'heads':[1,2,4,8], 'ffn_expansion_factor':2.66, 'bias':False, 'LayerNorm_type':'WithBias', 'dual_pixel_task':False}\n",
        "\n",
        "model = Restormer(**parameters)\n",
        "\n",
        "try:\n",
        "    opt = {'image_size': 224, 'pretrain_mae': PRETRAINED_WEIGHTS_PATH, 'device': DEVICE}\n",
        "    criterion = ReconstructLoss(opt)\n",
        "    model = model.cuda()\n",
        "    criterion.pretrain_mae = criterion.pretrain_mae.to(torch.device('cuda'))\n",
        "    print(\"Custom loss criterion initialized.\")\n",
        "except Exception as e: raise SystemExit(f\"Error initializing loss: {e}\")\n",
        "\n",
        "effective_lr = 5e-5 if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else LEARNING_RATE\n",
        "optimizer = optim.Adam(model.parameters(), lr=effective_lr, weight_decay=WEIGHT_DECAY)\n",
        "print(f\"Optimizer: Adam, LR: {effective_lr:.1e}, Weight Decay: {WEIGHT_DECAY:.1e}\")\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    num_halvings = epoch // LR_DECAY_EPOCHS\n",
        "    lr_multiplier = 0.5 ** num_halvings\n",
        "    final_multiplier = LR_FINAL / effective_lr # Use the actual starting LR\n",
        "    return max(lr_multiplier, final_multiplier)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "print(f\"LR Scheduler: Halve every {LR_DECAY_EPOCHS} epochs, min LR {LR_FINAL:.1e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- Starting Training for {EPOCHS} Epochs ---\")\n",
        "start_time = time.time()\n",
        "best_val_loss = 0.0\n",
        "SAVE_BEST_PATH = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_best_restormer_new.pth\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss_total = 0.0\n",
        "    epoch_loss_l1 = 0.0\n",
        "    epoch_loss_perc = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "\n",
        "        gt_img = sharp_imgs.to(DEVICE)\n",
        "        b_img = blur_imgs.to(DEVICE)\n",
        "        recover_img = model(b_img)\n",
        "        losses = criterion(recover_img, gt_img)\n",
        "\n",
        "        grad_loss = losses[\"total_loss\"]\n",
        "        optimizer.zero_grad()\n",
        "        grad_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_total += grad_loss.item()\n",
        "        epoch_loss_l1 += losses.get('l1', torch.tensor(0.0)).item()\n",
        "        epoch_loss_perc += losses.get('Perceptual', torch.tensor(0.0)).item()\n",
        "        progress_bar.set_postfix(loss=f\"{grad_loss.item():.4f}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss_total = epoch_loss_total / len(dataloader)\n",
        "    avg_loss_l1 = epoch_loss_l1 / len(dataloader)\n",
        "    avg_loss_perc = epoch_loss_perc / len(dataloader)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(\"Clearing cache before validation...\")\n",
        "    del gt_img, b_img, blur_imgs, sharp_imgs, gt_img, b_img, recover_img, losses, grad_loss\n",
        "    gc.collect()\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if run_validation and val_dataloader:\n",
        "\n",
        "        val_loss = validate_epoch(model, val_dataloader, DEVICE, criterion, visualize=True, num_images_to_show=4, epoch_num=None)\n",
        "\n",
        "        if val_loss > best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), SAVE_BEST_PATH)\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f} *** Best Model Saved ***\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f}\")\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation SKIPPED\")\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f})\")\n",
        "\n",
        "    current_checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "    save_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_loss_total,\n",
        "        'val_loss': val_loss, }\n",
        "    torch.save(save_data, current_checkpoint_path)\n",
        "\n",
        "status = \"Transferred weights\" if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else \"scratch\"\n",
        "final_model_path = os.path.join(OUTPUT_DIR, f\"{status}_customloss_restormer_epoch{EPOCHS}_final.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total Training Time: {total_time_str}\")\n",
        "print(f\"Final model saved to: {final_model_path}\")\n",
        "if run_validation and os.path.exists(SAVE_BEST_PATH):\n",
        "    print(f\"Best model (Validation PSNR: {best_val_loss:.4f}) saved to: {SAVE_BEST_PATH}\")\n",
        "elif run_validation:\n",
        "    print(f\"Best model not saved (Validation PSNR did not improve beyond initial {best_val_loss:.4f}).\")\n",
        "else:\n",
        "    print(\"Best model not saved (Validation was skipped).\")\n",
        "\n",
        "\n",
        "if 'dataset' in locals() and hasattr(dataset, 'close'): dataset.close()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "8gawrDpkV_V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1XNpFq3B3dM"
      },
      "source": [
        "##Training Logic for DeepDeblur model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth -O mae_pretrain_vit_base.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBpHYR4Li7p6",
        "outputId": "138e1c68-92b7-4a5a-9378-25b00d493786"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-12 00:12:12--  https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.123, 13.35.37.90, 13.35.37.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343249461 (327M) [binary/octet-stream]\n",
            "Saving to: ‘mae_pretrain_vit_base.pth’\n",
            "\n",
            "mae_pretrain_vit_ba 100%[===================>] 327.35M   237MB/s    in 1.4s    \n",
            "\n",
            "2026-01-12 00:12:13 (237 MB/s) - ‘mae_pretrain_vit_base.pth’ saved [343249461/343249461]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mcZmNX0jnjQ",
        "outputId": "fb9fcec5-8734-4d05-bd5d-9707c5645b19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TransformerPerceptualLoss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "T896BMtNUXbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43b4bdea-bb52-4cc8-882d-6f56b1094f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1308519218.py\", line 28, in <cell line: 0>\n",
            "    criterion = ReconstructLoss(opt)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/REDS/TransformerPerceptualLoss/loss/deblur_loss.py\", line 22, in __init__\n",
            "    self.pretrain_mae = mae_vit_base_patch16(img_size=img_size)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/TransformerPerceptualLoss/models/dual_model_mae.py\", line 242, in mae_vit_base_patch16_dec512d8b\n",
            "    model = MaskedAutoencoderViT(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/TransformerPerceptualLoss/models/dual_model_mae.py\", line 42, in __init__\n",
            "    Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
            "TypeError: Block.__init__() got an unexpected keyword argument 'qk_scale'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1308519218.py\", line 32, in <cell line: 0>\n",
            "    except Exception as e: raise SystemExit(f\"Error initializing loss: {e}\")\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "SystemExit: Error initializing loss: Block.__init__() got an unexpected keyword argument 'qk_scale'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1701, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1308519218.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pretrain_mae'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"/content/Restormer/mae_pretrain_vit_base.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReconstructLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/REDS/TransformerPerceptualLoss/loss/deblur_loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpretrained_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pretrain_mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae_vit_base_patch16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mpretrained_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TransformerPerceptualLoss/models/dual_model_mae.py\u001b[0m in \u001b[0;36mmae_vit_base_patch16_dec512d8b\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmae_vit_base_patch16_dec512d8b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     model = MaskedAutoencoderViT(\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TransformerPerceptualLoss/models/dual_model_mae.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size, patch_size, in_chans, embed_dim, depth, num_heads, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, norm_layer, norm_pix_loss)\u001b[0m\n\u001b[1;32m     41\u001b[0m         self.blocks = nn.ModuleList([\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             for i in range(depth)])\n",
            "\u001b[0;31mTypeError\u001b[0m: Block.__init__() got an unexpected keyword argument 'qk_scale'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1308519218.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Custom loss criterion initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error initializing loss: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: Error initializing loss: Block.__init__() got an unexpected keyword argument 'qk_scale'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "#Training DeepDeblur on Perceptual Loss\n",
        "\n",
        "import sys\n",
        "import os\n",
        "# Ensure the 'loss' directory from TransformerPerceptualLoss is in sys.path\n",
        "tp_root_path = '/content/TransformerPerceptualLoss'\n",
        "tp_loss_path = os.path.join(tp_root_path, 'loss')\n",
        "if tp_loss_path not in sys.path:\n",
        "    sys.path.insert(0, tp_loss_path) # Insert at the beginning for priority\n",
        "\n",
        "# Define PRETRAINED_WEIGHTS_PATH for effective_lr calculation\n",
        "PRETRAINED_WEIGHTS_PATH = \"/content/Restormer/mae_pretrain_vit_base.pth\" # Path to the downloaded MAE weights\n",
        "MODEL_NAME = 'DeepDeblur' # Define MODEL_NAME for this training block\n",
        "\n",
        "import deblur_loss # Changed from 'import loss.deblur_loss as deblur_loss'\n",
        "from deblur_loss import ReconstructPerceptualLoss as ReconstructLoss\n",
        "import importlib\n",
        "from utils import pos_embed\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block as TimmBlock\n",
        "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
        "from utils.pos_embed import get_2d_sincos_pos_embed\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "LOAD_PRETRAINED = True\n",
        "run_validation = True\n",
        "\n",
        "model = DeepDeblurMS()\n",
        "\n",
        "try:\n",
        "    opt = {'image_size': IMG_SIZE, 'pretrain_mae': PRETRAINED_WEIGHTS_PATH, 'device': DEVICE}\n",
        "    criterion = ReconstructLoss(opt)\n",
        "    model = model.cuda()\n",
        "    criterion.pretrain_mae = criterion.pretrain_mae.to(torch.device('cuda'))\n",
        "    print(\"Custom loss criterion initialized.\")\n",
        "except Exception as e: raise SystemExit(f\"Error initializing loss: {e}\")\n",
        "\n",
        "effective_lr = 5e-5 if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else LEARNING_RATE\n",
        "optimizer = optim.Adam(model.parameters(), lr=effective_lr, weight_decay=WEIGHT_DECAY)\n",
        "print(f\"Optimizer: Adam, LR: {effective_lr:.1e}, Weight Decay: {WEIGHT_DECAY:.1e}\")\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    num_halvings = epoch // LR_DECAY_EPOCHS\n",
        "    lr_multiplier = 0.5 ** num_halvings\n",
        "    final_multiplier = LR_FINAL / effective_lr # Use the actual starting LR\n",
        "    return max(lr_multiplier, final_multiplier)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "print(f\"LR Scheduler: Halve every {LR_DECAY_EPOCHS} epochs, min LR {LR_FINAL:.1e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- Starting Training for {EPOCHS} Epochs ---\")\n",
        "start_time = time.time()\n",
        "best_val_loss = 0.0\n",
        "SAVE_BEST_PATH = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_best_deepDeblur.pth\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss_total = 0.0\n",
        "    epoch_loss_l1 = 0.0\n",
        "    epoch_loss_perc = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "\n",
        "    for batch_idx, (blur_imgs, sharp_imgs) in enumerate(progress_bar):\n",
        "\n",
        "        gt_img = sharp_imgs.to(DEVICE)\n",
        "        b_img = blur_imgs.to(DEVICE)\n",
        "\n",
        "        sharp_gt_half = F.interpolate(gt_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter = F.interpolate(sharp_gt_half, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        fine_out, mid_out, coarse_out = model(b_img)\n",
        "\n",
        "        sharp_gt_half_224 = F.interpolate(sharp_gt_half, size = 224, mode='bilinear', align_corners=False)\n",
        "        sharp_gt_quarter_224 = F.interpolate(sharp_gt_quarter, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        mid_out_224 = F.interpolate(mid_out, size = 224, mode='bilinear', align_corners=False)\n",
        "        coarse_out_224 = F.interpolate(coarse_out, size = 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Uses ReconstructLoss's forward\n",
        "        loss_fine = criterion(fine_out, gt_img)\n",
        "        loss_mid = criterion(mid_out_224, sharp_gt_half_224)\n",
        "        loss_coarse = criterion(coarse_out_224, sharp_gt_quarter_224)\n",
        "        # Combining the fine, mid and coarse losses\n",
        "        losses = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "\n",
        "        # 4. Backpropagation\n",
        "        grad_loss = loss_fine[\"total_loss\"] + loss_mid[\"total_loss\"] + loss_coarse[\"total_loss\"]\n",
        "        optimizer.zero_grad()\n",
        "        grad_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        epoch_loss_total += grad_loss.item()\n",
        "        epoch_loss_l1 += loss_fine.get('l1', torch.tensor(0.0)).item() + loss_mid.get('l1', torch.tensor(0.0)).item() + loss_coarse.get('l1', torch.tensor(0.0)).item()\n",
        "        epoch_loss_perc += loss_fine.get('Perceptual', torch.tensor(0.0)).item() + loss_mid.get('Perceptual', torch.tensor(0.0)).item() + loss_coarse.get('Perceptual', torch.tensor(0.0)).item()\n",
        "        progress_bar.set_postfix(loss=f\"{grad_loss.item():.4f}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss_total = epoch_loss_total / len(dataloader)\n",
        "    avg_loss_l1 = epoch_loss_l1 / len(dataloader)\n",
        "    avg_loss_perc = epoch_loss_perc / len(dataloader)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(\"Clearing cache before validation...\")\n",
        "    del blur_imgs, sharp_imgs, gt_img, b_img, sharp_gt_half, sharp_gt_quarter, fine_out, mid_out, coarse_out, sharp_gt_half_224, sharp_gt_quarter_224, mid_out_224, coarse_out_224, losses, grad_loss\n",
        "    gc.collect()\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if run_validation and val_dataloader:\n",
        "\n",
        "        val_loss = validate_epoch(model, val_dataloader, DEVICE, criterion, visualize=True, num_images_to_show=4, epoch_num=None)\n",
        "\n",
        "        if val_loss > best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), SAVE_BEST_PATH)\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f} *** Best Model Saved ***\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation Loss: {val_loss:.4f}\")\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f}) || Validation SKIPPED\")\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - LR: {current_lr:.6f} - Loss: {avg_loss_total:.4f} (L1: {avg_loss_l1:.4f}, Perc: {avg_loss_perc:.4f})\")\n",
        "\n",
        "    current_checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "    save_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_loss_total,\n",
        "        'val_loss': val_loss, }\n",
        "    torch.save(save_data, current_checkpoint_path)\n",
        "\n",
        "status = \"Transferred weights\" if LOAD_PRETRAINED and os.path.exists(PRETRAINED_WEIGHTS_PATH) else \"scratch\"\n",
        "final_model_path = os.path.join(OUTPUT_DIR, f\"{MODEL_NAME}_{status}_customloss_deepDeblur_epoch{EPOCHS}_final.pth\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print(f\"\\n--- Training Finished ---\")\n",
        "print(f\"Total Training Time: {total_time_str}\")\n",
        "print(f\"Final model saved to: {final_model_path}\")\n",
        "if run_validation and os.path.exists(SAVE_BEST_PATH):\n",
        "    print(f\"Best model (Validation PSNR: {best_val_loss:.4f}) saved to: {SAVE_BEST_PATH}\")\n",
        "elif run_validation:\n",
        "    print(f\"Best model not saved (Validation PSNR did not improve beyond initial {best_val_loss:.4f}).\")\n",
        "else:\n",
        "    print(\"Best model not saved (Validation was skipped).\")\n",
        "\n",
        "\n",
        "if 'dataset' in locals() and hasattr(dataset, 'close'): dataset.close()\n",
        "print(\"Done.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L2K4drBNtrqe",
        "jRncBPYEbATz"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b656d1f885941ca9c54702135e7f17b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee4b27b0639449bba23baf7090353a03",
              "IPY_MODEL_40049daeb03144388c16deb3a9209053",
              "IPY_MODEL_72ccca726ad4423497fc1e01be0d1e37"
            ],
            "layout": "IPY_MODEL_a0a188b95bc047efb38d7c8df7ffb075"
          }
        },
        "ee4b27b0639449bba23baf7090353a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e28eba5cd694a75a69f8110546e9c70",
            "placeholder": "​",
            "style": "IPY_MODEL_8a86ab0af4624f268f397aac079af452",
            "value": "train_blur.zip:  48%"
          }
        },
        "40049daeb03144388c16deb3a9209053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81cc7e35b8fb42db8ab3072f2dccc37e",
            "max": 28287365503,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_120783e41b7f4ef986c6a185c65b8749",
            "value": 13611340834
          }
        },
        "72ccca726ad4423497fc1e01be0d1e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0624c5f3954d57bee57c4fe051839c",
            "placeholder": "​",
            "style": "IPY_MODEL_4817bb626918403d9d87b0cdbe785baf",
            "value": " 13.6G/28.3G [28:16&lt;01:44, 140MB/s]"
          }
        },
        "a0a188b95bc047efb38d7c8df7ffb075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e28eba5cd694a75a69f8110546e9c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a86ab0af4624f268f397aac079af452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81cc7e35b8fb42db8ab3072f2dccc37e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120783e41b7f4ef986c6a185c65b8749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b0624c5f3954d57bee57c4fe051839c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4817bb626918403d9d87b0cdbe785baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}